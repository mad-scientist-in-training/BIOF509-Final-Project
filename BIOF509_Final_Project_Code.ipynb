{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BIOF509 Final Project Code",
      "provenance": [],
      "authorship_tag": "ABX9TyOKWgVMXoM4TrtCHDUvNcJm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mad-scientist-in-training/BIOF509-Final-Project/blob/Project-Code/BIOF509_Final_Project_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npj7uN1fe2ey"
      },
      "source": [
        "# 1. import all necessary packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en4JOfqoddNk"
      },
      "source": [
        "# 2. import dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "os.environ['KAGGLEC_CONFIG_DIR'] = \"content/gdrive/My Drive/kaggle_dataset\"\n",
        "%cd /content/gdrive/My Drive/kaggle_dataset\n",
        "#copy the API command from kaggle and unzip the file\n",
        "!kaggle datasets download -d ruslankl/early-biomarkers-of-parkinsons-disease\n",
        "!unzip early-biomarkers-of-parkinsons-disease\n",
        "PD_df = pd.read_csv(r'dataset.csv')\n",
        "#optional - visualize dataset to understand various parameters before manipulation\n",
        "print(PD_df)\n",
        "len(PD_df)\n",
        "PD_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGMxvLCwdrDc"
      },
      "source": [
        "# 3. manipulate the dataset\n",
        "PD_df_1 = PD_df.drop(PD_df.columns[1:41], axis=1)\n",
        "PD_DF = PD_df_1.rename({' Participant  code ':'Participant_Type'}, axis=1)\n",
        "PD_DF['class'] = PD_DF.Participant_Type.str[0]\n",
        "target = PD_DF.pop('class')\n",
        "PD_DF.insert(0, 'class', target)\n",
        "PD_DF_new = PD_DF.drop(PD_DF.columns[1], axis=1)\n",
        "PD_DF_new[\"class\"] = PD_DF_new[\"class\"].replace('P', 0)\n",
        "PD_DF_new[\"class\"] = PD_DF_new[\"class\"].replace('H', 1)\n",
        "PD_DF_new[\"class\"] = PD_DF_new[\"class\"].replace('R', 2)\n",
        "col_names = list(PD_DF_new)\n",
        "PD_DF_new['class']\n",
        "col_names = list(PD_DF_new)\n",
        "df_scaled = PD_DF_new.copy()\n",
        "all_cols_no_class = df_scaled[col_names]\n",
        "all_cols_no_class = PD_DF_new.loc[:, PD_DF_new.columns !='class']\n",
        "scaler = StandardScaler().fit(all_cols_no_class.values)\n",
        "scaled_all_cols_no_class = scaler.transform(all_cols_no_class.values)\n",
        "df_scaled = pd.DataFrame(scaled_all_cols_no_class)\n",
        "df_scaled = pd.concat([df_scaled, PD_DF_new['class']], axis=1)\n",
        "df_scaled_new = df_scaled.drop(df_scaled.loc[df_scaled['class']==2].index)\n",
        "df_new_1 = df_scaled_new.rename(columns={'class':'24'})"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8smEiqDeCVq"
      },
      "source": [
        "# 4. create, train, test random forest classifier\n",
        "X = df_new_1.iloc[:, 0:23].values\n",
        "y = df_new_1.iloc[:, 24].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
        "classifier = RandomForestClassifier(max_depth=32, n_estimators=100)\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vQnj52J-VLO"
      },
      "source": [
        "# 5. measure the accuracy of the algorithm\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(accuracy_score(y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7di6ZbNGeKPR"
      },
      "source": [
        "# 5. optional: optimize parameters using GridSearchCV\n",
        "parameters = {\"n_estimators\":[5,10,50,100,250],\n",
        "              \"max_depth\":[2,4,8,16,32,None]}\n",
        "cv = GridSearchCV(classifier, parameters, cv=5)\n",
        "cv.fit(X_train, y_train)\n",
        "def display(results):\n",
        "  print(f'Best parameters are: {results.best_params_}')\n",
        "  print(\"\\n\")\n",
        "  mean_score = results.cv_results_['mean_test_score']\n",
        "  std_score = results.cv_results_['std_test_score']\n",
        "  params = results.cv_results_['params']\n",
        "  for mean, std, params in zip(mean_score, std_score, params):\n",
        "    print(f'{round(mean,4)} + or - {round(std,4)} for the {params}')\n",
        "display(cv)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}